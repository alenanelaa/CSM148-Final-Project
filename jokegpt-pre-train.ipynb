{"metadata":{"colab":{"collapsed_sections":["tFcSZXYKEWEU","GP-wbA1T5QHB","YIApZ0j_I_Z5"],"machine_shape":"hm","provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7046357,"sourceType":"datasetVersion","datasetId":4054750},{"sourceId":7055086,"sourceType":"datasetVersion","datasetId":4060701},{"sourceId":7055323,"sourceType":"datasetVersion","datasetId":4060880},{"sourceId":7076620,"sourceType":"datasetVersion","datasetId":4076139},{"sourceId":7086627,"sourceType":"datasetVersion","datasetId":4083065},{"sourceId":7102885,"sourceType":"datasetVersion","datasetId":4094569}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install comet_ml\nimport comet_ml\nfrom comet_ml import Experiment\nfrom comet_ml.integration.pytorch import log_model\n!pip install lightning\nimport lightning\nfrom lightning.fabric import Fabric\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nimport re\nimport os\nimport lzma\nfrom tqdm import tqdm\nimport mmap\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-12-02T05:08:03.175971Z","iopub.execute_input":"2023-12-02T05:08:03.176818Z","iopub.status.idle":"2023-12-02T05:08:30.746338Z","shell.execute_reply.started":"2023-12-02T05:08:03.176746Z","shell.execute_reply":"2023-12-02T05:08:30.745103Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: comet_ml in /opt/conda/lib/python3.10/site-packages (3.35.3)\nRequirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (4.19.0)\nRequirement already satisfied: psutil>=5.6.3 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (5.9.3)\nRequirement already satisfied: python-box<7.0.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (6.1.0)\nRequirement already satisfied: requests-toolbelt>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (0.10.1)\nRequirement already satisfied: requests>=2.18.4 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (2.31.0)\nRequirement already satisfied: semantic-version>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (2.10.0)\nRequirement already satisfied: sentry-sdk>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.34.0)\nRequirement already satisfied: simplejson in /opt/conda/lib/python3.10/site-packages (from comet_ml) (3.19.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.16.0)\nRequirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.26.15)\nRequirement already satisfied: websocket-client<1.4.0,>=0.55.0 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.3.3)\nRequirement already satisfied: wrapt>=1.11.2 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (1.15.0)\nRequirement already satisfied: wurlitzer>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (3.0.3)\nRequirement already satisfied: everett[ini]<3.2.0,>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (3.1.0)\nRequirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (0.21.6)\nRequirement already satisfied: rich>=13.3.2 in /opt/conda/lib/python3.10/site-packages (from comet_ml) (13.5.2)\nRequirement already satisfied: configobj in /opt/conda/lib/python3.10/site-packages (from everett[ini]<3.2.0,>=1.0.1->comet_ml) (5.0.8)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (23.1.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2023.7.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.30.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.9.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.18.4->comet_ml) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.18.4->comet_ml) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.18.4->comet_ml) (2023.7.22)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=13.3.2->comet_ml) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=13.3.2->comet_ml) (2.16.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: lightning in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning) (6.0.1)\nRequirement already satisfied: fsspec[http]<2025.0,>2021.06.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2023.10.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (0.9.0)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.24.3)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (21.3)\nRequirement already satisfied: torch<4.0,>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (2.0.0)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (1.2.0)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from lightning) (4.5.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning) (2.1.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (2.31.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (3.8.5)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.12.0->lightning) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.12.0->lightning) (2.1.3)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (2023.7.22)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.12.0->lightning) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"# fabric = Fabric(precision=\"16-mixed\")\n# device = fabric.device\ndevice = torch.device(\"cuda:1\")","metadata":{"execution":{"iopub.status.busy":"2023-12-02T05:08:30.748538Z","iopub.execute_input":"2023-12-02T05:08:30.748913Z","iopub.status.idle":"2023-12-02T05:08:30.755710Z","shell.execute_reply.started":"2023-12-02T05:08:30.748854Z","shell.execute_reply":"2023-12-02T05:08:30.754600Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{"_uuid":"08a492cb-dfda-4b86-b01e-3d81578b0f7e","_cell_guid":"ec76543c-1af2-4f42-8d07-5ede9b4585ff","id":"3ytLXnTZbXLq","trusted":true}},{"cell_type":"code","source":"# Redefining the clean_text function\ndef clean_text(text):\n    # Lowercase the text\n    text = text.lower()\n    # print(text, ' p1')\n    # Remove words that contain numbers\n    text = re.sub(r'\\w*\\d\\w*', '', text)\n\n    # remove digits longer than 5\n    text = re.sub(r'\\d{5,}', '', text)\n\n    # remove words longer than 20\n    text = re.sub(r'\\b\\w{21,}\\b', '', text)\n\n    # Preserve meaningful punctuation and numbers\n    text = re.sub(r\"[^a-z0-9.,!?;\\s']\", '', text)\n\n    # Remove words with characters other than letters in them\n    text = re.sub(r'\\s\\w+[.,!?;]\\w+\\s', ' ', text)\n\n    words = text.split()\n    text = ' '.join(words)\n\n    # Truncate text to start after the first period and end at the last period\n    start = text.find('.') + 1\n    end = text.rfind('.')\n    if start != 0 and end != -1 and end > start:\n        text = text[start:end].strip()\n\n    text = re.sub(r'\\,+[,\\s]+[^\\w]', ', ', text) # Remove occurences such as , ,\n    text = re.sub(r'\\.+[\\.\\s]+[^\\w]', '. ', text) # Remove occurences such as . .\n\n    text = re.sub(r'\\s+\\.', '.' , text) # If there is space before '.' remove it\n    text = re.sub(r'\\s+[,]', ',' , text) # If there is space before ',' remove it\n    text = re.sub(r'\\s+[!]', '!' , text) # If there is space before '!' remove it\n    text = re.sub(r'\\s+[?]', '?' , text) # If there is space before '?' remove it\n\n    # Remove unnecessary whitespaces and handle line breaks\n    text = text.strip()\n    text = re.sub(r'\\s+', ' ', text)\n\n    # Remove repeating commas and periods\n    text = re.sub(r'[.]+', '.', text)\n    text = re.sub(r'[,]+', ',', text)\n    text = re.sub(r'\\.+[,]+', ',', text)\n\n    return text\n\ndef is_valid_line(line):\n  if line:\n\n    if len(line) < 2:\n      return False\n\n    if not re.search('[a-z]', line):\n      return False\n\n    return True\n\n  return False","metadata":{"_uuid":"064a4cd6-5065-4ac6-82d6-b24e4ae3dfac","_cell_guid":"c7ce2c2d-e7e2-47a2-afa2-9f7a6b80e374","collapsed":false,"id":"MvER1-0YZx3X","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:30.756944Z","iopub.execute_input":"2023-12-02T05:08:30.757292Z","iopub.status.idle":"2023-12-02T05:08:30.780395Z","shell.execute_reply.started":"2023-12-02T05:08:30.757259Z","shell.execute_reply":"2023-12-02T05:08:30.779560Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# For instance, using the BERT tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nvocab_size = len(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-12-02T05:08:30.782313Z","iopub.execute_input":"2023-12-02T05:08:30.782614Z","iopub.status.idle":"2023-12-02T05:08:31.008576Z","shell.execute_reply.started":"2023-12-02T05:08:30.782587Z","shell.execute_reply":"2023-12-02T05:08:31.007678Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# #loading the vocab associated with mini pile\n# vocab_file_name = \"/kaggle/input/mini-pile/mini_pile_train_vocab.txt\"\n# with open(vocab_file_name, 'r' , encoding='utf-8') as f:\n#   chars = sorted(list(set(f.read())))\n#   f.close","metadata":{"_uuid":"dcd30a17-5377-45a4-8c36-e9789f17092d","_cell_guid":"42635ee0-e1a3-4f05-aa0c-74a560efc1fb","collapsed":false,"id":"s07RJ9TPOxwU","outputId":"a86b7b88-8b8d-435e-ac74-fa771e87da90","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:31.009924Z","iopub.execute_input":"2023-12-02T05:08:31.010337Z","iopub.status.idle":"2023-12-02T05:08:31.015721Z","shell.execute_reply.started":"2023-12-02T05:08:31.010299Z","shell.execute_reply":"2023-12-02T05:08:31.014484Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize","metadata":{"_uuid":"a7025618-4646-4a90-b800-7de5e263cc4d","_cell_guid":"4a018672-f24d-4994-b799-e80d63d5af09","id":"kOdq1BRpwKXv","trusted":true}},{"cell_type":"markdown","source":"## Character level encoding","metadata":{"_uuid":"cbde4fb1-3f12-4f8c-a9a7-fbb091d722fa","_cell_guid":"08e4c960-a722-4bbf-9e8a-f50cd959a4d2","id":"8cpld2bi5WvN","trusted":true}},{"cell_type":"code","source":"# stoi = { ch:i for i, ch in enumerate(chars)}\n# itos = { i:ch for i, ch in enumerate(chars)}\n\n# # encoder: string to int\n# encode = lambda s: [stoi[c] for c in s]\n\n# # decoder: int to string\n# decode = lambda l: ''.join([itos[i] for i in l])","metadata":{"_uuid":"98c0075a-0e29-46a2-998a-a5a35b522fec","_cell_guid":"6ff7b2e3-4129-4e77-9d2b-331424f6cb71","collapsed":false,"id":"fxx7gJPLJNsJ","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:31.016946Z","iopub.execute_input":"2023-12-02T05:08:31.017274Z","iopub.status.idle":"2023-12-02T05:08:31.033029Z","shell.execute_reply.started":"2023-12-02T05:08:31.017248Z","shell.execute_reply":"2023-12-02T05:08:31.032020Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{"_uuid":"228b9b10-319d-425a-8944-3bc4ff11a755","_cell_guid":"5b1cc3b0-3e33-426c-bd6b-92b6031d4abb","id":"_j91TWgg2aM-","trusted":true}},{"cell_type":"code","source":"# single head\nclass Head(nn.Module):\n\n  def __init__(self, head_size):\n    super().__init__()\n    self.key = nn.Linear(n_embd, head_size, bias=False)\n    self.query = nn.Linear(n_embd, head_size, bias=False)\n    self.value = nn.Linear(n_embd, head_size, bias=False)\n    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x):\n    B,T,C = x.shape\n    k = self.key(x)\n    q = self.query(x)\n\n    wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n    wei = F.softmax(wei, dim=-1)\n    wei = self.dropout(wei)\n\n    v = self.value(x)\n    out = wei @ v\n    return out\n\n# multi-head\nclass MultiHeadAttention(nn.Module):\n\n  def __init__(self, num_heads, head_size):\n    super().__init__()\n    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n    self.proj = nn.Linear(n_embd, n_embd)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self, x):\n    out = torch.cat([h(x) for h in self.heads], dim=-1)\n    out = self.dropout(self.proj(out))\n    return out\n\n\nclass FeedForward(nn.Module):\n\n  def __init__(self, n_embd):\n    super().__init__()\n    self.net = nn.Sequential(\n        nn.Linear(n_embd, 4 * n_embd),\n        nn.ReLU(),\n        nn.Linear(4 * n_embd, n_embd),\n        nn.Dropout(dropout),\n    )\n\n  def forward(self, x):\n    return self.net(x)\n\nclass Block(nn.Module):\n\n  def __init__(self, n_embd, n_head):\n    super().__init__()\n    head_size = n_embd // n_head\n    self.sa = MultiHeadAttention(n_head, head_size)\n    self.ffwd = FeedForward(n_embd)\n    self.ln1 = nn.LayerNorm(n_embd)\n    self.ln2 = nn.LayerNorm(n_embd)\n\n  def forward(self, x):\n    x = x + self.sa(self.ln1(x))\n    x = x + self.ffwd(self.ln2(x))\n    return x\n\n  '''\n  # post norm\n  def forward(self, x):\n    y = self.sa(x)\n    x = self.ln1(x + y)\n    y = self.ffwd(x)\n    x = self.ln2(x + y)\n    return x\n  '''","metadata":{"_uuid":"e2ea5dd1-f870-41da-a87b-51f825d577bc","_cell_guid":"3fd0bebf-b264-45c2-879a-4d75f5bdb035","collapsed":false,"id":"0nArmzUyTxLo","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:31.034276Z","iopub.execute_input":"2023-12-02T05:08:31.034604Z","iopub.status.idle":"2023-12-02T05:08:31.053469Z","shell.execute_reply.started":"2023-12-02T05:08:31.034578Z","shell.execute_reply":"2023-12-02T05:08:31.052360Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"class GPTLanguageModel(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n    self.ln_f = nn.LayerNorm(n_embd)\n    self.lm_head = nn.Linear(n_embd, vocab_size)\n\n  def forward(self, idx, targets=None):\n    B, T = idx.shape\n\n    tok_emb = self.token_embedding_table(idx)\n    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n    x = tok_emb + pos_emb\n    x = self.blocks(x)\n    x = self.ln_f(x)\n    logits = self.lm_head(x)\n\n    if targets is None:\n        loss = None\n    else:\n        B, T, C = logits.shape\n        logits = logits.view(B*T, C)\n        targets = targets.view(B*T)\n        loss = F.cross_entropy(logits, targets)\n\n    return logits, loss\n\n  def generate(self, idx, max_new_tokens):\n\n    for _ in range(max_new_tokens):\n        idx_cond = idx[:, -block_size:]\n        logits, loss = self(idx_cond)\n        logits = logits[:, -1, :]\n        probs = F.softmax(logits, dim=-1)\n        idx_next = torch.multinomial(probs, num_samples=1)\n        idx = torch.cat((idx, idx_next), dim=1)\n    return idx","metadata":{"_uuid":"c911ad24-cb7d-4831-a659-0af72be75f9f","_cell_guid":"d5a0d808-d6a4-485b-ac88-e802e24e4b6f","collapsed":false,"id":"PPPcEOQCT0e3","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:31.054826Z","iopub.execute_input":"2023-12-02T05:08:31.055246Z","iopub.status.idle":"2023-12-02T05:08:31.069952Z","shell.execute_reply.started":"2023-12-02T05:08:31.055159Z","shell.execute_reply":"2023-12-02T05:08:31.068941Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"# Batches","metadata":{"_uuid":"8f4b9ca8-bc61-44b4-a328-eb0648649946","_cell_guid":"a6c0c369-e448-4eec-9555-5f03915af115","id":"yud7dRn84n7v","trusted":true}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# memory map for using small snippets of text from a single file of any size\ndef get_random_chunk(split):\n    \n    if split == 'train': \n        filename = \"/kaggle/input/tiny-stories/tiny_stores_train.txt\" \n    else: \n        filename = \"/kaggle/input/tiny-stories/tiny_stores_val.txt\" \n        \n    with open(filename, 'rb') as f:\n        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n            # Determine the file size and a random position to start reading\n            file_size = len(mm)\n            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n\n            # Seek to the random position and read the block of text\n            mm.seek(start_pos)\n            block = mm.read(block_size*batch_size-1)\n\n            # if the data is not long enough\n            while len(block) - block_size <= 0:\n              block = mm.read(block_size*batch_size-1)\n\n            # Decode the block to a string, ignoring any invalid byte sequences\n            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n            # clean the data\n            decoded_block = clean_text(decoded_block)\n\n            # Train and test splits\n            data = torch.tensor(tokenizer.encode(decoded_block), dtype=torch.long)\n\n    return data\n\ndef get_batch(split):\n    data = get_random_chunk(split)\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n","metadata":{"execution":{"iopub.status.busy":"2023-12-02T05:08:31.071264Z","iopub.execute_input":"2023-12-02T05:08:31.071537Z","iopub.status.idle":"2023-12-02T05:08:31.089799Z","shell.execute_reply.started":"2023-12-02T05:08:31.071513Z","shell.execute_reply":"2023-12-02T05:08:31.088930Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# # data loading\n# def get_batch(split):\n#     # generate a small batch of data of inputs x and targets y\n#     data = train_data if split == 'train' else val_data\n#     ix = torch.randint(len(data) - block_size, (batch_size,))\n#     x = torch.stack([data[i:i+block_size] for i in ix])\n#     y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n#     x, y = x.to(device), y.to(device)\n#     return x, y","metadata":{"_uuid":"80daff66-165d-49fa-9b8f-ff75ba9f1054","_cell_guid":"20782aee-8bed-4784-9b3c-7451b5d5bd2a","collapsed":false,"id":"OnH-t9BijE_c","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:31.093734Z","iopub.execute_input":"2023-12-02T05:08:31.094081Z","iopub.status.idle":"2023-12-02T05:08:31.106323Z","shell.execute_reply.started":"2023-12-02T05:08:31.094052Z","shell.execute_reply":"2023-12-02T05:08:31.105235Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.mean().item()\n\n        out[split] = losses.mean()\n    model.train()\n    return out","metadata":{"_uuid":"7c70074c-da37-4eea-bf47-fdf3a7e267cd","_cell_guid":"7fdb3973-bc8b-4b57-9b23-ffe133449c84","collapsed":false,"id":"_Ih_QZE5fcxT","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:31.107722Z","iopub.execute_input":"2023-12-02T05:08:31.108050Z","iopub.status.idle":"2023-12-02T05:08:31.118955Z","shell.execute_reply.started":"2023-12-02T05:08:31.108016Z","shell.execute_reply":"2023-12-02T05:08:31.117962Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"_uuid":"6ba37aad-c32c-4e51-ad9c-084a9c64ff40","_cell_guid":"51f669c7-6d53-4a6d-9e55-df03d2790d7b","id":"pxhDFqrebzep","trusted":true}},{"cell_type":"markdown","source":"## Hyperparameters","metadata":{"_uuid":"1ed93097-680c-49b6-9f09-23543f02a218","_cell_guid":"3949befb-f236-4006-aeb4-a55aa546beaf","id":"CSV1U1wxl4Bq","trusted":true}},{"cell_type":"code","source":"# hyperparameters\nbatch_size = 128\nblock_size = 128\nmax_iters = 2000\neval_interval = 100\nlearning_rate = 3e-2\neval_iters = 2000\nn_embd = 200\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# device = fabric.device\ndevice = torch.device(\"cuda:1\")\n# vocab_size = len(chars)\nvocab_size = len(tokenizer)\n\nhyperparameters = {\n    'batch_size': batch_size,\n    'block_size': block_size,\n    'max_iters': max_iters,\n    'eval_interval': eval_interval,\n    'learning_rate': learning_rate,\n    'eval_iters': eval_iters,\n    'n_embd': n_embd,\n    'n_head': n_head,\n    'n_layer': n_layer,\n    'dropout': dropout,\n}\n\n# Hyperparameter -> Loss Visualization Data\n# Essentially for every run, record the hyperparams and\n# Plot them compared to old (save old in file)\nvis_data = {\n    'batch_size': batch_size,\n    'block_size': block_size,\n    'max_iters': max_iters,\n    'eval_interval': eval_interval,\n    'learning_rate': learning_rate,\n    'eval_iters': eval_iters,\n    'n_embd': n_embd,\n    'n_head': n_head,\n    'n_layer': n_layer,\n    'dropout': dropout,\n}\n\nvis_data_file_name = \"visual_data_during_training.txt\"\nvis_post_data_file_name = \"visual_data_post_training.txt\"","metadata":{"_uuid":"9aadd796-5c46-4e34-add5-5129ad1463d5","_cell_guid":"129c9e26-255e-4a22-a462-eab02c6ff0c4","collapsed":false,"id":"O95QHgGUT_4n","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:31.120286Z","iopub.execute_input":"2023-12-02T05:08:31.120709Z","iopub.status.idle":"2023-12-02T05:08:31.141038Z","shell.execute_reply.started":"2023-12-02T05:08:31.120668Z","shell.execute_reply":"2023-12-02T05:08:31.140151Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# model\nmodel = GPTLanguageModel()\nmodel.to(device)\n\nprint(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n\n# Visualization Data\nvis_data_iterations = [] # Format: [[iteration, train_loss, val_loss]]\n\n# optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)","metadata":{"_uuid":"de8722c8-23a1-448b-8aef-637fac9515b1","_cell_guid":"c8db6386-11c8-40e5-bc70-61bfaa392004","collapsed":false,"id":"_3az1gNpUe8B","outputId":"fec60ab7-2f2b-4131-a2f4-fadf3198b084","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:31.142178Z","iopub.execute_input":"2023-12-02T05:08:31.142471Z","iopub.status.idle":"2023-12-02T05:08:31.319515Z","shell.execute_reply.started":"2023-12-02T05:08:31.142445Z","shell.execute_reply":"2023-12-02T05:08:31.318419Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"14.193322 M parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"# paths\nbase_model_200_4_4_path = '/kaggle/input/base-models/base_model_200_4_4.pth'","metadata":{"_uuid":"b6ac9a30-de97-471c-85b2-83e5ef360d2a","_cell_guid":"93db99bf-0f4c-43e2-843a-82f7e8191909","collapsed":false,"id":"tJ1oZGMI1TRM","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:31.320764Z","iopub.execute_input":"2023-12-02T05:08:31.321069Z","iopub.status.idle":"2023-12-02T05:08:31.325788Z","shell.execute_reply.started":"2023-12-02T05:08:31.321043Z","shell.execute_reply":"2023-12-02T05:08:31.324707Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"# load the model and optimizer\npath = '/kaggle/input/base-models/base_model_200_4_4.pth'\ncheckpoint = torch.load(path)\nmodel.load_state_dict(checkpoint['model_state_dict'])\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","metadata":{"_uuid":"b6ac9a30-de97-471c-85b2-83e5ef360d2a","_cell_guid":"93db99bf-0f4c-43e2-843a-82f7e8191909","collapsed":false,"id":"tJ1oZGMI1TRM","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:31.327022Z","iopub.execute_input":"2023-12-02T05:08:31.327302Z","iopub.status.idle":"2023-12-02T05:08:31.573114Z","shell.execute_reply.started":"2023-12-02T05:08:31.327278Z","shell.execute_reply":"2023-12-02T05:08:31.572113Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2023-12-02T05:08:31.575016Z","iopub.execute_input":"2023-12-02T05:08:31.575479Z","iopub.status.idle":"2023-12-02T05:08:32.784943Z","shell.execute_reply.started":"2023-12-02T05:08:31.575447Z","shell.execute_reply":"2023-12-02T05:08:32.783818Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Sat Dec  2 05:08:32 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   69C    P0    32W /  70W |   4646MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla T4            Off  | 00000000:00:05.0 Off |                    0 |\n| N/A   70C    P0    33W /  70W |  14060MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\nexperiment = Experiment(\n  api_key=\"78u2AfbhkXeTChB3Kzb7FhOEY\",\n  project_name=\"JokeGPT\",\n  workspace=\"lzh0212\"\n)\n\nexperiment.log_parameters(hyperparameters)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n\nlosses = 0\nfor iter in range(max_iters):\n\n    xb, yb = get_batch('train')\n\n    # evaluate\n    logits, loss = model(xb, yb)\n    losses += loss.item()  # Use .item() to get the scalar value\n    \n    # Log and print at evaluation intervals\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        avg_loss = losses / (iter + 1)\n        current_lr = scheduler.get_last_lr()[0]  # Get current learning rate\n\n        print(f\"Iteration {iter}: Avg Loss = {avg_loss:.4f}, LR = {current_lr}\")\n\n        # Log metrics to experiment\n        experiment.log_metric(\"avg_loss\", avg_loss, step=iter)\n        experiment.log_metric(\"learning_rate\", current_lr, step=iter)\n\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    scheduler.step()\n\nexperiment.end()","metadata":{"_uuid":"cd7418a1-22ae-4b4f-887b-ffb0345a681d","_cell_guid":"51940b56-9c7b-46be-b57a-de0ac77a3ca0","id":"rRuissSBawv5","outputId":"76303bc2-d869-4948-f141-74031d4926d5","execution":{"iopub.status.busy":"2023-12-02T05:08:32.786417Z","iopub.execute_input":"2023-12-02T05:08:32.786712Z","iopub.status.idle":"2023-12-02T05:08:37.430784Z","shell.execute_reply.started":"2023-12-02T05:08:32.786684Z","shell.execute_reply":"2023-12-02T05:08:37.429065Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stderr","text":"\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/lzh0212/jokegpt/bdea953e876240e98b9552ffaeaaf47f\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size    : 128\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     block_size    : 128\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout       : 0.0\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_interval : 100\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_iters    : 2000\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate : 0.03\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_iters     : 2000\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_embd        : 200\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_head        : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_layer       : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/kaggle/working' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/lzh0212/jokegpt/6590c2743e9943829bb32e51a20af047\n\nToken indices sequence length is longer than the specified maximum sequence length for this model (4005 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[75], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m get_batch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# Use .item() to get the scalar value\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Log and print at evaluation intervals\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[66], line 18\u001b[0m, in \u001b[0;36mGPTLanguageModel.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x)\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[0;32m---> 18\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.86 GiB (GPU 1; 14.76 GiB total capacity; 13.39 GiB already allocated; 481.75 MiB free; 13.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.86 GiB (GPU 1; 14.76 GiB total capacity; 13.39 GiB already allocated; 481.75 MiB free; 13.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"markdown","source":"# Visualization","metadata":{"_uuid":"4a0f0d38-1fa7-4c67-a864-9ff31b2ab218","_cell_guid":"a1669e91-469c-4dcd-a34c-b4a8c277a48a","id":"o2qkkIasQ0vh","trusted":true}},{"cell_type":"markdown","source":"# Save the model","metadata":{"_uuid":"324352ff-d0b5-4478-b383-82fcb3f75983","_cell_guid":"e52ec7c5-f4ae-4e77-bc37-bc7214d84097","id":"NXMA91bMF9Un","trusted":true}},{"cell_type":"code","source":"def save(model, optimizer, hyperparameters, base_path='/kaggle/working/'):\n    n_embd = hyperparameters['n_embd']\n    n_head = hyperparameters['n_head']\n    n_layer = hyperparameters['n_layer']\n\n    filename = f\"base_model_{n_embd}_{n_head}_{n_layer}.pth\"\n    full_path = base_path + filename\n\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'hyperparameters': hyperparameters\n    }, full_path)\n\n    print(f\"Checkpoint saved to {full_path}\")","metadata":{"_uuid":"53db5f34-e4a4-4bbf-a42a-138abd9c307b","_cell_guid":"28f40b23-e67c-489c-9629-334fb5ac19fa","collapsed":false,"id":"opVU4Xin0-eK","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:37.432372Z","iopub.status.idle":"2023-12-02T05:08:37.432774Z","shell.execute_reply.started":"2023-12-02T05:08:37.432595Z","shell.execute_reply":"2023-12-02T05:08:37.432614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save(model, optimizer, hyperparameters)","metadata":{"_uuid":"0d6a4b84-e6b9-44a3-99d8-36245a18f7fc","_cell_guid":"e0e3f708-7caf-4461-93a2-b1dace2a8875","collapsed":false,"id":"Bq0Wh7kJndCF","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:37.434250Z","iopub.status.idle":"2023-12-02T05:08:37.434727Z","shell.execute_reply.started":"2023-12-02T05:08:37.434493Z","shell.execute_reply":"2023-12-02T05:08:37.434518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint['hyperparameters']","metadata":{"execution":{"iopub.status.busy":"2023-12-02T05:08:37.435927Z","iopub.status.idle":"2023-12-02T05:08:37.436353Z","shell.execute_reply.started":"2023-12-02T05:08:37.436140Z","shell.execute_reply":"2023-12-02T05:08:37.436160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result","metadata":{"_uuid":"a5a27436-acea-4a00-a6de-dfe7ac978fef","_cell_guid":"4d960604-51f5-47cf-8ecb-67f6b9a48b45","id":"-Ki9UU6A2d1A","trusted":true}},{"cell_type":"code","source":"prompt = 'hello'\nmodel = model.module\nmodel.to(device)\ncontext = torch.tensor(tokenizer.encode(prompt), dtype=torch.long, device=device)\ngenerated_chars = decode(model.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\nprint(generated_chars)","metadata":{"_uuid":"1efbec5e-db75-4c26-9f31-0ab6554d1f17","_cell_guid":"293c66a5-fca0-4e54-93a1-ad26f31bbfa0","collapsed":false,"id":"k6kfXKbWD1V4","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:37.440227Z","iopub.status.idle":"2023-12-02T05:08:37.441239Z","shell.execute_reply.started":"2023-12-02T05:08:37.441002Z","shell.execute_reply":"2023-12-02T05:08:37.441022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"context = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(model.generate(context, max_new_tokens=200)[0].tolist()))","metadata":{"_uuid":"5eee6b56-9ff0-4e88-850b-f047ef40eab9","_cell_guid":"d14a9681-9bca-4663-8f62-9d706dfc21ae","collapsed":false,"id":"JqYBg_SfNJa3","jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2023-12-02T05:08:37.444595Z","iopub.status.idle":"2023-12-02T05:08:37.445066Z","shell.execute_reply.started":"2023-12-02T05:08:37.444787Z","shell.execute_reply":"2023-12-02T05:08:37.444813Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/lzh0212/jokegpt/6590c2743e9943829bb32e51a20af047\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size    : 128\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     block_size    : 128\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     dropout       : 0.0\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_interval : 100\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_iters    : 2000\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate : 0.03\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     max_iters     : 2000\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_embd        : 200\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_head        : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     n_layer       : 4\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"_uuid":"bfa6ae4c-9c61-4d58-bce8-35d6b39ad329","_cell_guid":"e1568168-2fc7-4624-8275-e6204045c231","collapsed":false,"id":"_-XKa2l7bk1G","jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}