{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLgQvdLzeOnA",
        "outputId": "c788f163-d978-4f43-9461-596888a6f65d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting comet_ml\n",
            "  Downloading comet_ml-3.35.3-py3-none-any.whl (586 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.4/586.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (4.19.2)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (5.9.5)\n",
            "Collecting python-box<7.0.0 (from comet_ml)\n",
            "  Downloading python_box-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt>=0.8.0 (from comet_ml)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.31.0)\n",
            "Collecting semantic-version>=2.8.0 (from comet_ml)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting sentry-sdk>=1.1.0 (from comet_ml)\n",
            "  Downloading sentry_sdk-1.38.0-py2.py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simplejson (from comet_ml)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (2.0.7)\n",
            "Collecting websocket-client<1.4.0,>=0.55.0 (from comet_ml)\n",
            "  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (1.14.1)\n",
            "Collecting wurlitzer>=1.0.2 (from comet_ml)\n",
            "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
            "Collecting everett[ini]<3.2.0,>=1.0.1 (from comet_ml)\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6 (from comet_ml)\n",
            "  Downloading dulwich-0.21.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (512 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.2/512.2 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.10/dist-packages (from comet_ml) (13.7.0)\n",
            "Collecting configobj (from everett[ini]<3.2.0,>=1.0.1->comet_ml)\n",
            "  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (23.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2023.11.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.31.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.18.4->comet_ml) (2023.11.17)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.3.2->comet_ml) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n",
            "Installing collected packages: everett, wurlitzer, websocket-client, simplejson, sentry-sdk, semantic-version, python-box, dulwich, configobj, requests-toolbelt, comet_ml\n",
            "  Attempting uninstall: websocket-client\n",
            "    Found existing installation: websocket-client 1.6.4\n",
            "    Uninstalling websocket-client-1.6.4:\n",
            "      Successfully uninstalled websocket-client-1.6.4\n",
            "  Attempting uninstall: python-box\n",
            "    Found existing installation: python-box 7.1.1\n",
            "    Uninstalling python-box-7.1.1:\n",
            "      Successfully uninstalled python-box-7.1.1\n",
            "Successfully installed comet_ml-3.35.3 configobj-5.0.8 dulwich-0.21.6 everett-3.1.0 python-box-6.1.0 requests-toolbelt-1.0.0 semantic-version-2.10.0 sentry-sdk-1.38.0 simplejson-3.19.2 websocket-client-1.3.3 wurlitzer-3.0.3\n",
            "Collecting lightning\n",
            "  Downloading lightning-2.1.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.23.5)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (23.2)\n",
            "Requirement already satisfied: torch<4.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.1.0+cu118)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.5.0)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=1.12.0->lightning) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.12.0->lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.1.2 lightning-utilities-0.10.0 pytorch-lightning-2.1.2 torchmetrics-1.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install comet_ml\n",
        "import comet_ml\n",
        "from comet_ml import Experiment\n",
        "from comet_ml.integration.pytorch import log_model\n",
        "!pip install lightning\n",
        "import lightning\n",
        "from lightning.fabric import Fabric\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import re\n",
        "import os\n",
        "import lzma\n",
        "from tqdm import tqdm\n",
        "import mmap\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHu99-1UYErd",
        "outputId": "1e49c30d-92a2-4cff-ffa4-36699dd39aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using 16-bit Automatic Mixed Precision (AMP)\n"
          ]
        }
      ],
      "source": [
        "fabric = Fabric(precision=\"16-mixed\")\n",
        "device = fabric.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97pWWwNdUuVO",
        "outputId": "1ececc19-913b-4fd2-b5c8-3d59b3fec927"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFcSZXYKEWEU"
      },
      "source": [
        "# Loading Jokes dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss0tvhkmqGb5"
      },
      "source": [
        "# Files loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EXsyx8dkShP"
      },
      "source": [
        "## mini-en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdQbcQ6alNqX",
        "outputId": "1693c209-904a-4979-be29-b2c7e39f3df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Project\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3oFtNfZtKt_"
      },
      "outputs": [],
      "source": [
        "# file_name = \"mini-en.txt\"\n",
        "# text = \"\"\n",
        "# with open(file_name, 'r', encoding='utf-8') as f:\n",
        "#   text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDSz3JwetS5f"
      },
      "outputs": [],
      "source": [
        "# chars = sorted(list(set(text)))\n",
        "# vocab_size = len(chars)\n",
        "# print(''.join(chars))\n",
        "# print(vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC8ZHEm0JC4r"
      },
      "source": [
        "## tiny stories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ytLXnTZbXLq"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNWUU5AoNjxt"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove words that contain numbers\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "    # remove digits longer than 5\n",
        "    text = re.sub(r'\\d{5,}', '', text)\n",
        "\n",
        "    # remove words longer than 20\n",
        "    text = re.sub(r'\\b\\w{21,}\\b', '', text)\n",
        "\n",
        "    # Preserve meaningful punctuation and numbers\n",
        "    text = re.sub(r\"[^a-z0-9.,!?;'\\s]\", '', text)\n",
        "\n",
        "\n",
        "    #normalize whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    return text.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvER1-0YZx3X"
      },
      "outputs": [],
      "source": [
        "# # Redefining the clean_text function\n",
        "# def clean_text(text):\n",
        "#     # Lowercase the text\n",
        "#     text = text.lower()\n",
        "#     # print(text, ' p1')\n",
        "#     # Remove words that contain numbers\n",
        "#     text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "#     # remove digits longer than 5\n",
        "#     text = re.sub(r'\\d{5,}', '', text)\n",
        "\n",
        "#     # remove words longer than 20\n",
        "#     text = re.sub(r'\\b\\w{21,}\\b', '', text)\n",
        "\n",
        "#     # Preserve meaningful punctuation and numbers\n",
        "#     text = re.sub(r\"[^a-z0-9.,!?;\\s']\", '', text)\n",
        "\n",
        "#     # Remove words with characters other than letters in them\n",
        "#     text = re.sub(r'\\s\\w+[.,!?;]\\w+\\s', ' ', text)\n",
        "\n",
        "#     words = text.split()\n",
        "#     text = ' '.join(words)\n",
        "\n",
        "#     # Truncate text to start after the first period and end at the last period\n",
        "#     start = text.find('.') + 1\n",
        "#     end = text.rfind('.')\n",
        "#     if start != 0 and end != -1 and end > start:\n",
        "#         text = text[start:end].strip()\n",
        "\n",
        "#     text = re.sub(r'\\,+[,\\s]+[^\\w]', ', ', text) # Remove occurences such as , ,\n",
        "#     text = re.sub(r'\\.+[\\.\\s]+[^\\w]', '. ', text) # Remove occurences such as . .\n",
        "\n",
        "#     text = re.sub(r'\\s+\\.', '.' , text) # If there is space before '.' remove it\n",
        "#     text = re.sub(r'\\s+[,]', ',' , text) # If there is space before ',' remove it\n",
        "#     text = re.sub(r'\\s+[!]', '!' , text) # If there is space before '!' remove it\n",
        "#     text = re.sub(r'\\s+[?]', '?' , text) # If there is space before '?' remove it\n",
        "\n",
        "#     text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
        "#     text = re.sub(r'\\S+@\\S+', '', text) # Remove email addresses\n",
        "#     text = re.sub(r'.*;\\s*$', '', text, flags=re.MULTILINE) # Remove lines that look like code (e.g., ending with semicolon)\n",
        "\n",
        "#     # Remove unnecessary whitespaces and handle line breaks\n",
        "#     text = text.strip()\n",
        "#     text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "#     # Remove repeating commas and periods\n",
        "#     text = re.sub(r'[.]+', '.', text)\n",
        "#     text = re.sub(r'[,]+', ',', text)\n",
        "#     text = re.sub(r'\\.+[,]+', ',', text)\n",
        "\n",
        "#     return text\n",
        "\n",
        "# def is_valid_line(line):\n",
        "#   if line:\n",
        "\n",
        "#     if len(line) < 2:\n",
        "#       return False\n",
        "\n",
        "#     if not re.search('[a-z]', line):\n",
        "#       return False\n",
        "\n",
        "#     return True\n",
        "\n",
        "#   return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DcKgn9K7gsu",
        "outputId": "6afd469c-16fb-4f17-a864-d59660076e2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1DUoPwqF9OmnYJmuAwgDAFNsyXvHQQS9l/Project/mini-en\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Project/mini-en"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D12NE2JNsRdY"
      },
      "outputs": [],
      "source": [
        "# cleaned_file_name = \"mini-en-cleaned-updated-fin.txt\"\n",
        "# with open(cleaned_file_name, 'r' , encoding='utf-8') as f:\n",
        "#   chars = sorted(list(set(f.read())))\n",
        "#   f.close()\n",
        "# vocab_size = len(chars)\n",
        "# print(''.join(chars))\n",
        "# print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2dNBP_AMNBO"
      },
      "outputs": [],
      "source": [
        "cleaned_lines = []\n",
        "\n",
        "with open('/content/drive/MyDrive/Project/tiny-stories/TinyStories-train.txt', 'r') as f:\n",
        "    for line in f:\n",
        "        cleaned_line = clean_text(line)  # Replace 'clean_text' with your actual cleaning function\n",
        "        cleaned_lines.append(cleaned_line)\n",
        "\n",
        "cleaned_text2 = ''.join(cleaned_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2j5_wrtF-Cvn",
        "outputId": "be906b81-7c8a-49bd-d41e-78d1aaceaa44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " !',.;?abcdefghijklmnopqrstuvwxyz\n",
            "33\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(cleaned_text2)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpXrYNBfhfa0"
      },
      "outputs": [],
      "source": [
        "with open('vocab_tiny_stories.txt', 'w') as f:\n",
        "    f.write(''.join(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvoVp9dZjC_8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "outputId": "fccbdec6-74d3-4f29-8676-776559cb122a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-13bed5ae7c17>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load tiny storie vocab into chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vocab_tiny_stories.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mchars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'vocab_tiny_stories.txt'"
          ]
        }
      ],
      "source": [
        "#load tiny storie vocab into chars\n",
        "with open('vocab_tiny_stories.txt', 'r') as f:\n",
        "    chars = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLt13uszWU4e",
        "outputId": "1f6b6be9-9d3f-414b-f4c3-b84f32be7837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !',.;?abcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ],
      "source": [
        "chars = ''\n",
        "with open(\"/content/drive/MyDrive/Project/Minipile/mini_pile_train_vocab.txt\", 'r') as f:\n",
        "  chars = f.read()\n",
        "print(\"\".join(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H26lGcF7ZFAd"
      },
      "outputs": [],
      "source": [
        "#to run with mini pile\n",
        "#clean the test, train and validation mini-pile\n",
        "%cd /content/drive/MyDrive/Minipile\n",
        "\n",
        "# import os\n",
        "# import re\n",
        "\n",
        "# # Redefining the clean_text function\n",
        "# def clean_text(text):\n",
        "#     # Lowercase the text\n",
        "#     text = text.lower()\n",
        "#     # print(text, ' p1')\n",
        "#     # Remove words that contain numbers\n",
        "#     text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "\n",
        "#     # remove digits longer than 5\n",
        "#     text = re.sub(r'\\d{5,}', '', text)\n",
        "\n",
        "#     # Preserve meaningful punctuation and numbers\n",
        "#     text = re.sub(r\"[^a-z0-9.,!?;\\s']\", '', text)\n",
        "\n",
        "#     # Remove words with characters other than letters in them\n",
        "#     text = re.sub(r'\\s\\w+[.,!?;]\\w+\\s', ' ', text)\n",
        "\n",
        "#     words = text.split()\n",
        "#     text = ' '.join(words)\n",
        "\n",
        "#     # Truncate text to start after the first period and end at the last period\n",
        "#     start = text.find('.') + 1\n",
        "#     end = text.rfind('.')\n",
        "#     if start != 0 and end != -1 and end > start:\n",
        "#         text = text[start:end].strip()\n",
        "\n",
        "#     text = re.sub(r'\\,+[,\\s]+[^\\w]', ', ', text) # Remove occurences such as , ,\n",
        "#     text = re.sub(r'\\.+[\\.\\s]+[^\\w]', '. ', text) # Remove occurences such as . .\n",
        "\n",
        "#     text = re.sub(r'\\s+\\.', '.' , text) # If there is space before '.' remove it\n",
        "#     text = re.sub(r'\\s+[,]', ',' , text) # If there is space before ',' remove it\n",
        "#     text = re.sub(r'\\s+[!]', '!' , text) # If there is space before '!' remove it\n",
        "#     text = re.sub(r'\\s+[?]', '?' , text) # If there is space before '?' remove it\n",
        "#     text = re.sub(r'http\\S+', '', text) # Remove URLs\n",
        "#     text = re.sub(r'\\S+@\\S+', '', text) # Remove email addresses\n",
        "#     text = re.sub(r'.*;\\s*$', '', text, flags=re.MULTILINE) # Remove lines that look like code (e.g., ending with semicolon)\n",
        "\n",
        "#     # Remove unnecessary whitespaces and handle line breaks\n",
        "#     text = text.strip()\n",
        "#     text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "#     # Remove repeating commas and periods\n",
        "#     text = re.sub(r'[.]+', '.', text)\n",
        "#     text = re.sub(r'[,]+', ',', text)\n",
        "#     text = re.sub(r'\\.+[,]+', ',', text)\n",
        "\n",
        "#     return text\n",
        "\n",
        "# def is_valid_line(line):\n",
        "#   if line:\n",
        "\n",
        "#     if len(line) < 2:\n",
        "#       return False\n",
        "\n",
        "#     if not re.search('[a-z]', line):\n",
        "#       return False\n",
        "\n",
        "#     return True\n",
        "\n",
        "#   return False\n",
        "\n",
        "\n",
        "# def process_chunk(chunk, out_file):\n",
        "#     invalid_count = 0\n",
        "#     for line in chunk:\n",
        "#         line = clean_text(line)\n",
        "#         if is_valid_line(line):\n",
        "#             out_file.write(line + '\\n')\n",
        "#         else:\n",
        "#             invalid_count += 1\n",
        "#     return invalid_count\n",
        "\n",
        "\n",
        "# file_name = \"mini_pile_train.txt\"\n",
        "# chunk_size = 10000\n",
        "# total_invalid_count = 0\n",
        "\n",
        "# cleaned_file_name = \"mini_pile_train_cleaned_v2.txt\"\n",
        "# with open(file_name, 'r', encoding='utf-8') as f, open(cleaned_file_name, 'w') as out_file:\n",
        "#     while True:\n",
        "#         lines = [f.readline() for _ in range(chunk_size)]\n",
        "#         lines = [line for line in lines if line]  # Filter out empty lines\n",
        "\n",
        "#         if not lines:\n",
        "#             break  # Exit loop if no more lines to read\n",
        "\n",
        "#         invalid_count = process_chunk(lines, out_file)\n",
        "#         total_invalid_count += invalid_count\n",
        "\n",
        "# print(f\"Processed and cleaned the file. Total invalid lines skipped: {total_invalid_count}. Clean data written to {cleaned_file_name}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuEEBiN2GcF0"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/Project/Minipile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb4H-aqJon2p"
      },
      "outputs": [],
      "source": [
        "# file_name = \"mini_pile_train_cleaned_v2.txt\"\n",
        "# text = \"\"\n",
        "# with open(file_name, 'r', encoding='utf-8') as f:\n",
        "#   text = f.read()\n",
        "\n",
        "# chars = sorted(list(set(text)))\n",
        "# vocab_size = len(chars)\n",
        "# print(''.join(chars))\n",
        "# print(vocab_size)\n",
        "# #save the cleaned chars as file_name_vocab.txt\n",
        "# with open('mini_pile_train_cleaned_v2_vocab.txt', 'w', encoding='utf-8') as f:\n",
        "#   f.write(''.join(chars))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dR19l-398jA"
      },
      "source": [
        "## vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s07RJ9TPOxwU",
        "outputId": "5f40acc9-bcf9-4c93-d28e-edcfd3839e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Project/Minipile\n",
            "34\n",
            "\n",
            " !',.;?abcdefghijklmnopqrstuvwxyz\n"
          ]
        }
      ],
      "source": [
        "#loading the vocab associated with mini pile\n",
        "#%cd /content/drive/MyDrive/Minipile\n",
        "# %cd /content/drive/MyDrive/Project/Minipile\n",
        "# vocab_file_name = \"mini_pile_train_cleaned_v2_vocab.txt\"\n",
        "# with open(vocab_file_name, 'r' , encoding='utf-8') as f:\n",
        "#   chars = sorted(list(set(f.read())))\n",
        "#   f.close\n",
        "\n",
        "# print(len(chars))\n",
        "# print(''.join(chars))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOdq1BRpwKXv"
      },
      "source": [
        "# Tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cpld2bi5WvN"
      },
      "source": [
        "## Basic encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxx7gJPLJNsJ"
      },
      "outputs": [],
      "source": [
        "stoi = { ch:i for i, ch in enumerate(chars)}\n",
        "itos = { i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# encoder: string to int\n",
        "encode = lambda s: [stoi[c] for c in s if c in stoi]\n",
        "\n",
        "# decoder: int to string\n",
        "decode = lambda l: ''.join([itos[i] for i in l if i in itos])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j91TWgg2aM-"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nArmzUyTxLo"
      },
      "outputs": [],
      "source": [
        "# single head\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "    wei = self.dropout(wei)\n",
        "\n",
        "    v = self.value(x)\n",
        "    out = wei @ v\n",
        "    return out\n",
        "\n",
        "# multi-head\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.proj = nn.Linear(n_embd, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))\n",
        "    return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, 4 * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * n_embd, n_embd),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "  def __init__(self, n_embd, n_head):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // n_head\n",
        "    self.sa = MultiHeadAttention(n_head, head_size)\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln1 = nn.LayerNorm(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(self.ln1(x))\n",
        "    x = x + self.ffwd(self.ln2(x))\n",
        "    return x\n",
        "\n",
        "  '''\n",
        "  # post norm\n",
        "  def forward(self, x):\n",
        "    y = self.sa(x)\n",
        "    x = self.ln1(x + y)\n",
        "    y = self.ffwd(x)\n",
        "    x = self.ln2(x + y)\n",
        "    return x\n",
        "  '''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPPcEOQCT0e3"
      },
      "outputs": [],
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "    self.ln_f = nn.LayerNorm(n_embd)\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    B, T = idx.shape\n",
        "\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)\n",
        "    x = self.ln_f(x)\n",
        "    logits = self.lm_head(x)\n",
        "\n",
        "    if targets is None:\n",
        "        loss = None\n",
        "    else:\n",
        "        B, T, C = logits.shape\n",
        "        logits = logits.view(B*T, C)\n",
        "        targets = targets.view(B*T)\n",
        "        loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, idx, max_new_tokens):\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -block_size:]\n",
        "        logits, loss = self(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yud7dRn84n7v"
      },
      "source": [
        "# Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN8SkSLNB3lO",
        "outputId": "19d3ca8f-4893-4571-9d25-d9abf9af7502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wl4symiU4p0K"
      },
      "outputs": [],
      "source": [
        "# memory map for using small snippets of text from a single file of any size\n",
        "def get_random_chunk(split):\n",
        "    filename = \"TinyStories-train.txt\" if split == 'train' else \"TinyStories-valid.txt\"\n",
        "    with open(filename, 'rb') as f:\n",
        "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
        "            # Determine the file size and a random position to start reading\n",
        "            file_size = len(mm)\n",
        "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
        "\n",
        "            # Seek to the random position and read the block of text\n",
        "            mm.seek(start_pos)\n",
        "            block = mm.read(block_size*batch_size-1)\n",
        "\n",
        "            # if the data is not long enough\n",
        "            while len(block) - block_size <= 0:\n",
        "              block = mm.read(block_size*batch_size-1)\n",
        "\n",
        "            # Decode the block to a string, ignoring any invalid byte sequences\n",
        "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
        "            # clean the data\n",
        "            # decoded_block = clean_text(decoded_block)\n",
        "            # Do we need to clean the decoded block?\n",
        "\n",
        "            # Train and test splits\n",
        "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
        "\n",
        "    return data\n",
        "\n",
        "def get_batch(split):\n",
        "    data = get_random_chunk(split)\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA9uxbeyRxi6"
      },
      "outputs": [],
      "source": [
        "# #use mini pile data - load and encode in batches\n",
        "# #this blows up the CUDA RAM\n",
        "\n",
        "# def data_generator(file_name, chunk_size=512):\n",
        "#     with open(file_name, 'r', encoding='utf-8') as file:\n",
        "#         while True:\n",
        "#             data = file.read(chunk_size)\n",
        "#             if not data:\n",
        "#                 break\n",
        "#             yield torch.tensor(encode(data), dtype=torch.long, device=device)\n",
        "\n",
        "# # Usage\n",
        "# generator = data_generator(cleaned_file_name, chunk_size=1024)  # Adjust chunk_size as needed\n",
        "\n",
        "# # Initialize lists to store data\n",
        "# train_data_list = []\n",
        "# val_data_list = []\n",
        "\n",
        "# # Process each chunk and accumulate\n",
        "# for data_chunk in generator:\n",
        "#     n = int(0.9 * len(data_chunk))  # Splitting each chunk into 90% train and 10% validation\n",
        "#     train_data_list.append(data_chunk[:n])\n",
        "#     val_data_list.append(data_chunk[n:])\n",
        "\n",
        "\n",
        "\n",
        "# # Concatenate all chunks into single tensors\n",
        "# train_data = torch.cat(train_data_list)\n",
        "# val_data = torch.cat(val_data_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmlQyl6m1CB4"
      },
      "outputs": [],
      "source": [
        "# Train and test splits for mini en\n",
        "# cleaned_file_name = \"mini_pile_train_cleaned.txt\"\n",
        "# with open(cleaned_file_name, 'r', encoding='utf-8') as f:\n",
        "#   data = torch.tensor(encode(f.read()), dtype=torch.long, device=device)\n",
        "#   n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "#   train_data = data[:n]\n",
        "#   val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OnH-t9BijE_c"
      },
      "outputs": [],
      "source": [
        "# # data loading\n",
        "# def get_batch(split):\n",
        "#     # generate a small batch of data of inputs x and targets y\n",
        "#     data = train_data if split == 'train' else val_data\n",
        "#     ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "#     x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "#     y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "#     x, y = x.to(device), y.to(device)\n",
        "#     return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ih_QZE5fcxT"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxhDFqrebzep"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSV1U1wxl4Bq"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O95QHgGUT_4n"
      },
      "outputs": [],
      "source": [
        "# hyperparameters\n",
        "batch_size = 512\n",
        "block_size = 128\n",
        "max_iters = 1000\n",
        "eval_interval = 200\n",
        "learning_rate = 3e-5\n",
        "eval_iters = 2000\n",
        "n_embd = 200\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "#device = torch.device(\"cuda:0\")\n",
        "device = fabric.device\n",
        "vocab_size = len(chars)\n",
        "\n",
        "hyperparameters = {\n",
        "    'batch_size': batch_size,\n",
        "    'block_size': block_size,\n",
        "    'max_iters': max_iters,\n",
        "    'eval_interval': eval_interval,\n",
        "    'learning_rate': learning_rate,\n",
        "    'eval_iters': eval_iters,\n",
        "    'n_embd': n_embd,\n",
        "    'n_head': n_head,\n",
        "    'n_layer': n_layer,\n",
        "    'dropout': dropout,\n",
        "}\n",
        "\n",
        "# Hyperparameter -> Loss Visualization Data\n",
        "# Essentially for every run, record the hyperparams and\n",
        "# Plot them compared to old (save old in file)\n",
        "vis_data = {\n",
        "    'batch_size': batch_size,\n",
        "    'block_size': block_size,\n",
        "    'max_iters': max_iters,\n",
        "    'eval_interval': eval_interval,\n",
        "    'learning_rate': learning_rate,\n",
        "    'eval_iters': eval_iters,\n",
        "    'n_embd': n_embd,\n",
        "    'n_head': n_head,\n",
        "    'n_layer': n_layer,\n",
        "    'dropout': dropout,\n",
        "}\n",
        "\n",
        "vis_data_file_name = \"visual_data_during_training.txt\"\n",
        "vis_post_data_file_name = \"visual_data_post_training.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3az1gNpUe8B",
        "outputId": "709aa089-9555-4e09-9c89-3f65d3de28ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.967233 M parameters\n"
          ]
        }
      ],
      "source": [
        "# model\n",
        "model = GPTLanguageModel()\n",
        "# model.to(fabric.device)\n",
        "model.to(device)\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# Visualization Data\n",
        "vis_data_iterations = [] # Format: [[iteration, train_loss, val_loss]]\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7mtwlfM4bf0",
        "outputId": "90d1be98-d82e-4003-94af-83c77629e16c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "tJ1oZGMI1TRM",
        "outputId": "477c7206-ba59-46d8-ddc2-35488bd9ccd2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"# load the model and optimizer\\n\\nmodel_384_8_8_path = '/content/drive/MyDrive/Project/Saved model/model_384_8_8.pth'\\nmodel_384_16_16_path = '/content/drive/MyDrive/Project/Saved model/model_384_16_16.pth'\\nmodel_128_4_4_path = '/content/drive/MyDrive/Project/Saved model/model_128_4_4.pth'\\n\\ncheckpoint = torch.load(model_384_8_8_path)\\ncheckpoint = torch.load(model_384_16_16_path)\\nmodel.load_state_dict(checkpoint['model_state_dict'])\\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\\n\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''# load the model and optimizer\n",
        "\n",
        "model_384_8_8_path = '/content/drive/MyDrive/Project/Saved model/model_384_8_8.pth'\n",
        "model_384_16_16_path = '/content/drive/MyDrive/Project/Saved model/model_384_16_16.pth'\n",
        "model_128_4_4_path = '/content/drive/MyDrive/Project/Saved model/model_128_4_4.pth'\n",
        "\n",
        "checkpoint = torch.load(model_384_8_8_path)\n",
        "checkpoint = torch.load(model_384_16_16_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mliMbIm2nh_z"
      },
      "outputs": [],
      "source": [
        "model_128_4_4_path = '/content/drive/MyDrive/Project/Saved Model/model_128_4_4.pth'\n",
        "checkpoint = torch.load(model_128_4_4_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqIIJ5ko-QWK",
        "outputId": "c5e2548b-6949-4309-95df-9ddd409ec156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/.shortcut-targets-by-id/10m65ZjTAPRJQ-QSuZpZdpGEB2CTwLxJi/tiny-stories\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Project/tiny-stories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7IsJLiAwB5fb",
        "outputId": "5d4b8df6-85bf-4fb6-b111-47628807d3aa"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/root'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIRKJz2bBlsx"
      },
      "outputs": [],
      "source": [
        "model_128_8_8_path = '/content/drive/MyDrive/Project/Saved Model/model_128_8_8_updated.pth'\n",
        "checkpoint = torch.load(model_128_8_8_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tTR-VfYEehS"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "fabric.launch()\n",
        "\n",
        "experiment = Experiment(\n",
        "  api_key=\"78u2AfbhkXeTChB3Kzb7FhOEY\",\n",
        "  project_name=\"JokeGPT\",\n",
        "  workspace=\"lzh0212\"\n",
        ")\n",
        "\n",
        "experiment.log_parameters(hyperparameters)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "  # other code...\n",
        "\n",
        "\n",
        "  if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "    losses = estimate_loss()\n",
        "    experiment.log_metric(\"train_loss\", losses['train'], epoch=iter)\n",
        "    experiment.log_metric(\"val_loss\", losses['val'], epoch=iter)\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "\n",
        "# after the training loop\n",
        "experiment.end()\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRuissSBawv5",
        "outputId": "11368766-2ca2-4cbb-e7a0-1f694beb0561"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/content/drive/.shortcut-targets-by-id/10m65ZjTAPRJQ-QSuZpZdpGEB2CTwLxJi/tiny-stories' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/isfahanisasan/general/27b28582c9b446638540022f4615f669\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loss at 0: 1.9033\n",
            "step 0: train loss 1.8666, val loss 1.8651\n",
            "loss at 200: 1.8570\n",
            "step 200: train loss 1.8180, val loss 1.8171\n",
            "loss at 400: 1.7499\n",
            "step 400: train loss 1.7719, val loss 1.7711\n",
            "loss at 600: 1.7724\n",
            "step 600: train loss 1.7335, val loss 1.7322\n",
            "loss at 800: 1.6901\n"
          ]
        }
      ],
      "source": [
        "fabric.launch()\n",
        "\n",
        "experiment = Experiment(\n",
        "  api_key=\"8D1CVm6D2zRnIoB9RCQghv2V0\",\n",
        "  project_name=\"general\",\n",
        "  workspace=\"isfahanisasan\"\n",
        ")\n",
        "\n",
        "experiment.log_parameters(hyperparameters)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "  # if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "    # losses = estimate_loss()\n",
        "    # vis_data_iterations.append([iter, losses['train'], losses['val']])\n",
        "    # print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb, yb = get_batch('train')\n",
        "\n",
        "  # evaluate\n",
        "  logits, loss = model(xb, yb)\n",
        "  if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "    print(f\"loss at {iter}: {loss:.4f}\")\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "    losses = estimate_loss()\n",
        "    experiment.log_metric(\"train_loss\", losses['train'], epoch=iter)\n",
        "    experiment.log_metric(\"val_loss\", losses['val'], epoch=iter)\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "log_model(experiment, model, model_name=\"Model 1\")\n",
        "experiment.end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOf157Jk4Ott"
      },
      "outputs": [],
      "source": [
        "log_model(experiment, model, model_name=\"Model 1\")\n",
        "experiment.end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "c10KVVpVpURQ",
        "outputId": "a40d26a3-156e-4cd0-9c25-28f7ea7976f4"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-23b3b9d50aa5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvis_data_iterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-2b0b4391c5f4>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "losses = estimate_loss()\n",
        "vis_data_iterations.append([max_iters-1, losses['train'].numpy(), losses['val'].numpy()])\n",
        "print(f\"train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJxc-numpULA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2qkkIasQ0vh"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "wFqkJPEBLjyG",
        "outputId": "14332b4d-0bae-4cc8-9720-2ce3be010827"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6599b28f7247>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mline_entries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mval_x_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_entries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mval_y_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_entries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-6599b28f7247>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mline_entries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mval_x_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_entries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mval_y_points\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline_entries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'tensor(0.8692)'"
          ]
        }
      ],
      "source": [
        "#Visualization -> graph performance based on different hyperparameters\n",
        "\n",
        "#PRE-TRAINING\n",
        "# Todo:\n",
        "  # Write vis_data_iterations to file and plot it\n",
        "  # Write vis_data to file and plot it\n",
        "  # READ FROM FILES TO PLOT PAST DATA!\n",
        "\n",
        "# Dont uncomment... vis_data_iterations = [[3,4,5]]\n",
        "\n",
        "val_x_points = []\n",
        "val_y_points = []\n",
        "entries = []\n",
        "\n",
        "# just added an extra open to guarantee that file is created\n",
        "# if it does not exist...\n",
        "with open(vis_data_file_name, 'a' , encoding='utf-8') as f:\n",
        "  for entry in vis_data_iterations:\n",
        "    entry_str = ' '.join(str(i) for i in entry) + '\\n'\n",
        "\n",
        "  f.write(entry_str)\n",
        "  f.close()\n",
        "\n",
        "with open(vis_data_file_name, 'r' , encoding='utf-8') as f:\n",
        "  lines = f.read().split('\\n')\n",
        "\n",
        "  for line in lines:\n",
        "    if not line:\n",
        "      continue\n",
        "\n",
        "    entries.append(line)\n",
        "\n",
        "  entries = set(entries)\n",
        "\n",
        "  for line in entries:\n",
        "    line_entries = line.split(' ')\n",
        "    val_x_points.append(int(line_entries[0]))\n",
        "    val_y_points.append([float(i) for i in line_entries[1:]])\n",
        "\n",
        "  f.close()\n",
        "\n",
        "with open(vis_data_file_name, 'w' , encoding='utf-8') as f:\n",
        "  f.write('\\n'.join(entries) + '\\n')\n",
        "  f.close()\n",
        "\n",
        "val_x_points = np.array(val_x_points)\n",
        "val_y_points = np.array(val_y_points)\n",
        "\n",
        "plt.plot(val_x_points, val_y_points)\n",
        "\n",
        "plt.title(\"Losses over Iterations\")\n",
        "plt.xlabel(\"Iteration #\")\n",
        "plt.ylabel(\"Training and Validation Loss\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#JOKE TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUQuz5E4SleN"
      },
      "outputs": [],
      "source": [
        "vis_data[\"loss\"] = estimate_loss()\n",
        "\n",
        "# ADD THE FEATURES YOU WANT TO PLOT LOSS FOR, EACH ONE GENERATES GRAPH FOR LOSS!\n",
        "hyper_plots = ['batch_size', 'block_size', 'max_iters', 'learning_rate', 'n_layer']\n",
        "# NOTE: All other props have to remain constant for plot\n",
        "# I.e. if plotting loss for \"batch_size\", everything else (e.g \"max_iters\")\n",
        "# has to remain the same, else could not accurately plot. That is all the other\n",
        "# props have to match the ones defined in the hyperparameters section.\n",
        "\n",
        "# Essentially it will plot specified feature compared to loss when all the\n",
        "# other features are equal to the entries in hyperparameters.\n",
        "\n",
        "# Plot them compared to old (save old in file)\n",
        "\n",
        "data_lists = {}\n",
        "\n",
        "for plot_type in hyper_plots:\n",
        "  data_lists[plot_type] = {\n",
        "      'x_points' : [],\n",
        "      'y_points' : [],\n",
        "  }\n",
        "\n",
        "def process_dict(entry):\n",
        "  for plot_type in hyper_plots:\n",
        "    valid_point = True\n",
        "\n",
        "    for item in hyperparameters.items():\n",
        "      if item[0] == plot_type:\n",
        "        continue\n",
        "      if item[1] != entry[item[0]]:\n",
        "        valid_point = False\n",
        "        break\n",
        "\n",
        "    if valid_point:\n",
        "      data_lists[plot_type]['x_points'].append(entry[plot_type])\n",
        "      data_lists[plot_type]['y_points'].append(entry['loss'])\n",
        "\n",
        "\n",
        "\n",
        "hyper_str = str(vis_data)\n",
        "\n",
        "with open(vis_post_data_file_name, 'a' , encoding='utf-8') as f:\n",
        "  f.write(hyper_str+'\\n')\n",
        "  f.close()\n",
        "\n",
        "with open(vis_post_data_file_name, 'r' , encoding='utf-8') as f:\n",
        "  entries = []\n",
        "  lines = f.read().split('\\n')\n",
        "\n",
        "  for line in lines:\n",
        "    if not line:\n",
        "      continue\n",
        "    entries.append(line)\n",
        "\n",
        "  entries = set(entries)\n",
        "\n",
        "  for line in entries:\n",
        "    entry_dict = eval(line)\n",
        "    process_dict(entry_dict)\n",
        "\n",
        "  f.close()\n",
        "\n",
        "with open(vis_post_data_file_name, 'w' , encoding='utf-8') as f:\n",
        "  f.write('\\n'.join(entries) + '\\n')\n",
        "  f.close()\n",
        "\n",
        "for plot_type in hyper_plots:\n",
        "  x_points = np.array(data_lists[plot_type]['x_points'])\n",
        "  y_points = np.array(data_lists[plot_type]['y_points'])\n",
        "\n",
        "  plt.plot(x_points, y_points)\n",
        "\n",
        "  plt.title(f'Losses over {plot_type}')\n",
        "  plt.xlabel(plot_type)\n",
        "  plt.ylabel(f'{plot_type} to Loss')\n",
        "\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXMA91bMF9Un"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opVU4Xin0-eK"
      },
      "outputs": [],
      "source": [
        "def save(model, optimizer, hyperparameters, base_path='/content/drive/MyDrive/Project/Saved Model/'):\n",
        "    n_embd = hyperparameters['n_embd']\n",
        "    n_head = hyperparameters['n_head']\n",
        "    n_layer = hyperparameters['n_layer']\n",
        "\n",
        "    filename = f\"model_{n_embd}_{n_head}_{n_layer}.pth\"\n",
        "    full_path = base_path + filename\n",
        "\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'hyperparameters': hyperparameters\n",
        "    }, full_path)\n",
        "\n",
        "    print(f\"Checkpoint saved to {full_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq0Wh7kJndCF",
        "outputId": "0e1d2598-7666-4d0e-853d-dbb6946bd0d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved to /content/drive/MyDrive/Project/Saved Model/model_200_4_4.pth\n"
          ]
        }
      ],
      "source": [
        "save(model, optimizer, hyperparameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ki9UU6A2d1A"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6kfXKbWD1V4",
        "outputId": "600370ab-437c-4fb9-faf6-32f35fd86d85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "helloilk how mand mjap joked her to to noing up. he cat apims put sframeed the! laih?tig her they the sald buleaghtep hougham. she githeed nomeed, borre.lon.ton dex he platy to we aid subottege trime ry. h\n"
          ]
        }
      ],
      "source": [
        "prompt = 'hello'\n",
        "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
        "generated_chars = decode(model.generate(context.unsqueeze(0), max_new_tokens=200)[0].tolist())\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqYBg_SfNJa3",
        "outputId": "4ca2dbb7-62eb-438d-90a0-622f21fc9dc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  and mayourn, it anke ta dabmy, stetten's sad hat ance i he sus plool her was. of pim on a shey to couch ass.the to soulland antee died dand and anded sorsid bicket pugs.the day weard lemit.et ple she\n"
          ]
        }
      ],
      "source": [
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(model.generate(context, max_new_tokens=200)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-XKa2l7bk1G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "tFcSZXYKEWEU"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}